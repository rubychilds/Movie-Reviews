{
 "metadata": {
  "name": "",
  "signature": "sha256:31d53251a280544359c3db2ce85f223835271b60cd7f8c3b8e09e87f786eace6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import movie_reviews\n",
      "from random import shuffle\n",
      "import string\n",
      "import re\n",
      "from operator import itemgetter\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn import metrics\n",
      "from sklearn.metrics import classification_report\n",
      "import nltk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 100
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "docs = [(movie_reviews.raw(fileid), category)\n",
      "    for category in movie_reviews.categories()\n",
      "    for fileid in movie_reviews.fileids(category)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "clean_docs = [(re.sub('\\\\n|\\\\t',' ',doc).translate(string.maketrans(\"\",\"\"), string.punctuation).lower(), label) for doc,label in docs]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clean_docs[1:2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "[('the happy bastards quick movie review  damn that y2k bug   its got a head start in this movie starring jamie lee curtis and another baldwin brother  william this time  in a story regarding a crew of a tugboat that comes across a deserted russian tech ship that has a strangeness to it when they kick the power back on   little do they know the power within     going for the gore and bringing on a few action sequences here and there  virus still feels very empty  like a movie going for all flash and no substance   we dont know why the crew was really out in the middle of nowhere  we dont know the origin of what took over the ship  just that a big pink flashy thing hit the mir   and  of course  we dont know why donald sutherland is stumbling around drunkenly throughout   here  its just  hey  lets chase these people around with some robots    the acting is below average  even from the likes of curtis   youre more likely to get a kick out of her work in halloween h20   sutherland is wasted and baldwin  well  hes acting like a baldwin  of course   the real star here are stan winstons robot design  some schnazzy cgi  and the occasional good gore shot  like picking into someones brain   so  if robots and body parts really turn you on  heres your movie   otherwise  its pretty much a sunken ship of a movie   ',\n",
        "  'neg')]"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# SkiLearn Multinomial Classifier\n",
      "This method requires us to split the examples from the labels. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "positive_reviews = [doc for doc,label in clean_docs if label == 'pos']\n",
      "negative_reviews = [doc for doc,label in clean_docs if label == 'neg']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we define the training size and test size for the data. We have used an 80% training, 20% test split."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "training_size = 0.8 \n",
      "test_size = 1 - training_size"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "positive_training_size = int(len(positive_reviews)*training_size)\n",
      "negative_training_size = int(len(negative_reviews)*training_size)\n",
      "positive_labels = [1]*len(positive_reviews)\n",
      "negative_labels = [0]*len(negative_reviews)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Below we now split the data using the sizes calculated above."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# TRAINING\n",
      "positive_training_examples = positive_reviews[:positive_training_size]\n",
      "positive_training_labels = positive_labels[:positive_training_size]\n",
      "\n",
      "negative_training_examples = negative_reviews[:negative_training_size]\n",
      "negative_training_labels =negative_labels[:negative_training_size]\n",
      "\n",
      "# TEST\n",
      "positive_test_examples = positive_reviews[positive_training_size:]\n",
      "positive_test_labels = positive_labels[positive_training_size:]\n",
      "\n",
      "negative_test_examples = negative_reviews[negative_training_size:]\n",
      "negative_test_labels = negative_labels[negative_training_size:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "training_examples = positive_training_examples + negative_training_examples\n",
      "test_examples = positive_test_examples + negative_test_examples\n",
      "training_labels = positive_training_labels + negative_training_labels\n",
      "test_labels = positive_test_labels + negative_test_labels"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For vectorization of the data we ensure the data is in the expected numpy array format."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = np.array([''.join(el) for el in training_examples])\n",
      "X_test = np.array([''.join(el) for el in test_examples]) \n",
      "y_train = np.array(training_labels)\n",
      "y_test = np.array(test_labels) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Tfidf stands for term-frequency inverse-document frequency. This is a way of understanding a words statistical relevance and importance in a collection of documents. Here we define two statistics, term-frquency tf(t,d) where we look at the raw frequency within a document. \n",
      "\n",
      "[Alt text](http://upload.wikimedia.org/math/5/c/c/5cc18acd4dbd9be636047fc2a7a10105.png)\n",
      "\n",
      "\n",
      "Inverse-document frequency meaures the amount of information a word provides. For this we look at the word across all documents.\n",
      "\n",
      "[Alt text](http://upload.wikimedia.org/math/b/a/e/bae842b33a4cafc0f22519cf960b052a.png)\n",
      "\n",
      "min_df allows us to ignore terms with frequency less than two. This allows us to exclude extremely rare words, which maybe considered as noise. ngram_range allows us to extract the ngrams which can be n-tuples of words. We set this to have a minimum of 1 to view frequencies of individual words. The maximum is two to pick up on pairings which may commonly occur together. By adding a parameter for stopwords, we remove all commonly occuring english words, similarly we can pass a list onto this."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english', strip_accents='unicode', norm='l2',decode_error=\"ignore\")\n",
      "# analyzer=stemming_analyzer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_string = unicode(training_examples[0])\n",
      "print \"Example string: \" + test_string\n",
      "print \"Preprocessed string: \" + vectorizer.build_preprocessor()(test_string)\n",
      "print \"Tokenized string:\" + str(vectorizer.build_tokenizer()(test_string))\n",
      "print \"N-gram data string:\" + str(vectorizer.build_analyzer()(test_string))\n",
      "print \"\\n\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Example string: films adapted from comic books have had plenty of success  whether theyre about superheroes  batman  superman  spawn   or geared toward kids  casper  or the arthouse crowd  ghost world   but theres never really been a comic book like from hell before   for starters  it was created by alan moore  and eddie campbell   who brought the medium to a whole new level in the mid 80s with a 12part series called the watchmen   to say moore and campbell thoroughly researched the subject of jack the ripper would be like saying michael jackson is starting to look a little odd   the book  or  graphic novel   if you will  is over 500 pages long and includes nearly 30 more that consist of nothing but footnotes   in other words  dont dismiss this film because of its source   if you can get past the whole comic book thing  you might find another stumbling block in from hells directors  albert and allen hughes   getting the hughes brothers to direct this seems almost as ludicrous as casting carrot top in  well  anything  but riddle me this  who better to direct a film thats set in the ghetto and features really violent street crime than the mad geniuses behind menace ii society   the ghetto in question is  of course  whitechapel in 1888 londons east end   its a filthy  sooty place where the whores  called  unfortunates   are starting to get a little nervous about this mysterious psychopath who has been carving through their profession with surgical precision   when the first stiff turns up  copper peter godley  robbie coltrane  the world is not enough  calls in inspector frederick abberline  johnny depp  blow  to crack the case   abberline  a widower  has prophetic dreams he unsuccessfully tries to quell with copious amounts of absinthe and opium   upon arriving in whitechapel  he befriends an unfortunate named mary kelly  heather graham  say it isnt so  and proceeds to investigate the horribly gruesome crimes that even the police surgeon cant stomach   i dont think anyone needs to be briefed on jack the ripper  so i wont go into the particulars here  other than to say moore and campbell have a unique and interesting theory about both the identity of the killer and the reasons he chooses to slay   in the comic  they dont bother cloaking the identity of the ripper  but screenwriters terry hayes  vertical limit  and rafael yglesias  les mis  rables  do a good job of keeping him hidden from viewers until the very end   its funny to watch the locals blindly point the finger of blame at jews and indians because  after all  an englishman could never be capable of committing such ghastly acts   and from hells ending had me whistling the stonecutters song from the simpsons for days   who holds back the electric carwho made steve guttenberg a star      dont worry  itll all make sense when you see it   now onto from hells appearance  its certainly dark and bleak enough  and its surprising to see how much more it looks like a tim burton film than planet of the apes did  at times  it seems like sleepy hollow 2    the print i saw wasnt completely finished  both color and music had not been finalized  so no comments about marilyn manson   but cinematographer peter deming  dont say a word  ably captures the dreariness of victorianera london and helped make the flashy killing scenes remind me of the crazy flashbacks in twin peaks  even though the violence in the film pales in comparison to that in the blackandwhite comic   oscar winner martin childs  shakespeare in love  production design turns the original prague surroundings into one creepy place   even the acting in from hell is solid  with the dreamy depp turning in a typically strong performance and deftly handling a british accent   ians holm  joe goulds secret  and richardson  102 dalmatians  log in great supporting roles  but the big surprise here is graham   i cringed the first time she opened her mouth  imagining her attempt at an irish accent  but it actually wasnt half bad   the film  however  is all good   2  00  r for strong violencegore  sexuality  language and drug content  \n",
        "Preprocessed string: films adapted from comic books have had plenty of success  whether theyre about superheroes  batman  superman  spawn   or geared toward kids  casper  or the arthouse crowd  ghost world   but theres never really been a comic book like from hell before   for starters  it was created by alan moore  and eddie campbell   who brought the medium to a whole new level in the mid 80s with a 12part series called the watchmen   to say moore and campbell thoroughly researched the subject of jack the ripper would be like saying michael jackson is starting to look a little odd   the book  or  graphic novel   if you will  is over 500 pages long and includes nearly 30 more that consist of nothing but footnotes   in other words  dont dismiss this film because of its source   if you can get past the whole comic book thing  you might find another stumbling block in from hells directors  albert and allen hughes   getting the hughes brothers to direct this seems almost as ludicrous as casting carrot top in  well  anything  but riddle me this  who better to direct a film thats set in the ghetto and features really violent street crime than the mad geniuses behind menace ii society   the ghetto in question is  of course  whitechapel in 1888 londons east end   its a filthy  sooty place where the whores  called  unfortunates   are starting to get a little nervous about this mysterious psychopath who has been carving through their profession with surgical precision   when the first stiff turns up  copper peter godley  robbie coltrane  the world is not enough  calls in inspector frederick abberline  johnny depp  blow  to crack the case   abberline  a widower  has prophetic dreams he unsuccessfully tries to quell with copious amounts of absinthe and opium   upon arriving in whitechapel  he befriends an unfortunate named mary kelly  heather graham  say it isnt so  and proceeds to investigate the horribly gruesome crimes that even the police surgeon cant stomach   i dont think anyone needs to be briefed on jack the ripper  so i wont go into the particulars here  other than to say moore and campbell have a unique and interesting theory about both the identity of the killer and the reasons he chooses to slay   in the comic  they dont bother cloaking the identity of the ripper  but screenwriters terry hayes  vertical limit  and rafael yglesias  les mis  rables  do a good job of keeping him hidden from viewers until the very end   its funny to watch the locals blindly point the finger of blame at jews and indians because  after all  an englishman could never be capable of committing such ghastly acts   and from hells ending had me whistling the stonecutters song from the simpsons for days   who holds back the electric carwho made steve guttenberg a star      dont worry  itll all make sense when you see it   now onto from hells appearance  its certainly dark and bleak enough  and its surprising to see how much more it looks like a tim burton film than planet of the apes did  at times  it seems like sleepy hollow 2    the print i saw wasnt completely finished  both color and music had not been finalized  so no comments about marilyn manson   but cinematographer peter deming  dont say a word  ably captures the dreariness of victorianera london and helped make the flashy killing scenes remind me of the crazy flashbacks in twin peaks  even though the violence in the film pales in comparison to that in the blackandwhite comic   oscar winner martin childs  shakespeare in love  production design turns the original prague surroundings into one creepy place   even the acting in from hell is solid  with the dreamy depp turning in a typically strong performance and deftly handling a british accent   ians holm  joe goulds secret  and richardson  102 dalmatians  log in great supporting roles  but the big surprise here is graham   i cringed the first time she opened her mouth  imagining her attempt at an irish accent  but it actually wasnt half bad   the film  however  is all good   2  00  r for strong violencegore  sexuality  language and drug content  \n",
        "Tokenized string:[u'films', u'adapted', u'from', u'comic', u'books', u'have', u'had', u'plenty', u'of', u'success', u'whether', u'theyre', u'about', u'superheroes', u'batman', u'superman', u'spawn', u'or', u'geared', u'toward', u'kids', u'casper', u'or', u'the', u'arthouse', u'crowd', u'ghost', u'world', u'but', u'theres', u'never', u'really', u'been', u'comic', u'book', u'like', u'from', u'hell', u'before', u'for', u'starters', u'it', u'was', u'created', u'by', u'alan', u'moore', u'and', u'eddie', u'campbell', u'who', u'brought', u'the', u'medium', u'to', u'whole', u'new', u'level', u'in', u'the', u'mid', u'80s', u'with', u'12part', u'series', u'called', u'the', u'watchmen', u'to', u'say', u'moore', u'and', u'campbell', u'thoroughly', u'researched', u'the', u'subject', u'of', u'jack', u'the', u'ripper', u'would', u'be', u'like', u'saying', u'michael', u'jackson', u'is', u'starting', u'to', u'look', u'little', u'odd', u'the', u'book', u'or', u'graphic', u'novel', u'if', u'you', u'will', u'is', u'over', u'500', u'pages', u'long', u'and', u'includes', u'nearly', u'30', u'more', u'that', u'consist', u'of', u'nothing', u'but', u'footnotes', u'in', u'other', u'words', u'dont', u'dismiss', u'this', u'film', u'because', u'of', u'its', u'source', u'if', u'you', u'can', u'get', u'past', u'the', u'whole', u'comic', u'book', u'thing', u'you', u'might', u'find', u'another', u'stumbling', u'block', u'in', u'from', u'hells', u'directors', u'albert', u'and', u'allen', u'hughes', u'getting', u'the', u'hughes', u'brothers', u'to', u'direct', u'this', u'seems', u'almost', u'as', u'ludicrous', u'as', u'casting', u'carrot', u'top', u'in', u'well', u'anything', u'but', u'riddle', u'me', u'this', u'who', u'better', u'to', u'direct', u'film', u'thats', u'set', u'in', u'the', u'ghetto', u'and', u'features', u'really', u'violent', u'street', u'crime', u'than', u'the', u'mad', u'geniuses', u'behind', u'menace', u'ii', u'society', u'the', u'ghetto', u'in', u'question', u'is', u'of', u'course', u'whitechapel', u'in', u'1888', u'londons', u'east', u'end', u'its', u'filthy', u'sooty', u'place', u'where', u'the', u'whores', u'called', u'unfortunates', u'are', u'starting', u'to', u'get', u'little', u'nervous', u'about', u'this', u'mysterious', u'psychopath', u'who', u'has', u'been', u'carving', u'through', u'their', u'profession', u'with', u'surgical', u'precision', u'when', u'the', u'first', u'stiff', u'turns', u'up', u'copper', u'peter', u'godley', u'robbie', u'coltrane', u'the', u'world', u'is', u'not', u'enough', u'calls', u'in', u'inspector', u'frederick', u'abberline', u'johnny', u'depp', u'blow', u'to', u'crack', u'the', u'case', u'abberline', u'widower', u'has', u'prophetic', u'dreams', u'he', u'unsuccessfully', u'tries', u'to', u'quell', u'with', u'copious', u'amounts', u'of', u'absinthe', u'and', u'opium', u'upon', u'arriving', u'in', u'whitechapel', u'he', u'befriends', u'an', u'unfortunate', u'named', u'mary', u'kelly', u'heather', u'graham', u'say', u'it', u'isnt', u'so', u'and', u'proceeds', u'to', u'investigate', u'the', u'horribly', u'gruesome', u'crimes', u'that', u'even', u'the', u'police', u'surgeon', u'cant', u'stomach', u'dont', u'think', u'anyone', u'needs', u'to', u'be', u'briefed', u'on', u'jack', u'the', u'ripper', u'so', u'wont', u'go', u'into', u'the', u'particulars', u'here', u'other', u'than', u'to', u'say', u'moore', u'and', u'campbell', u'have', u'unique', u'and', u'interesting', u'theory', u'about', u'both', u'the', u'identity', u'of', u'the', u'killer', u'and', u'the', u'reasons', u'he', u'chooses', u'to', u'slay', u'in', u'the', u'comic', u'they', u'dont', u'bother', u'cloaking', u'the', u'identity', u'of', u'the', u'ripper', u'but', u'screenwriters', u'terry', u'hayes', u'vertical', u'limit', u'and', u'rafael', u'yglesias', u'les', u'mis', u'rables', u'do', u'good', u'job', u'of', u'keeping', u'him', u'hidden', u'from', u'viewers', u'until', u'the', u'very', u'end', u'its', u'funny', u'to', u'watch', u'the', u'locals', u'blindly', u'point', u'the', u'finger', u'of', u'blame', u'at', u'jews', u'and', u'indians', u'because', u'after', u'all', u'an', u'englishman', u'could', u'never', u'be', u'capable', u'of', u'committing', u'such', u'ghastly', u'acts', u'and', u'from', u'hells', u'ending', u'had', u'me', u'whistling', u'the', u'stonecutters', u'song', u'from', u'the', u'simpsons', u'for', u'days', u'who', u'holds', u'back', u'the', u'electric', u'carwho', u'made', u'steve', u'guttenberg', u'star', u'dont', u'worry', u'itll', u'all', u'make', u'sense', u'when', u'you', u'see', u'it', u'now', u'onto', u'from', u'hells', u'appearance', u'its', u'certainly', u'dark', u'and', u'bleak', u'enough', u'and', u'its', u'surprising', u'to', u'see', u'how', u'much', u'more', u'it', u'looks', u'like', u'tim', u'burton', u'film', u'than', u'planet', u'of', u'the', u'apes', u'did', u'at', u'times', u'it', u'seems', u'like', u'sleepy', u'hollow', u'the', u'print', u'saw', u'wasnt', u'completely', u'finished', u'both', u'color', u'and', u'music', u'had', u'not', u'been', u'finalized', u'so', u'no', u'comments', u'about', u'marilyn', u'manson', u'but', u'cinematographer', u'peter', u'deming', u'dont', u'say', u'word', u'ably', u'captures', u'the', u'dreariness', u'of', u'victorianera', u'london', u'and', u'helped', u'make', u'the', u'flashy', u'killing', u'scenes', u'remind', u'me', u'of', u'the', u'crazy', u'flashbacks', u'in', u'twin', u'peaks', u'even', u'though', u'the', u'violence', u'in', u'the', u'film', u'pales', u'in', u'comparison', u'to', u'that', u'in', u'the', u'blackandwhite', u'comic', u'oscar', u'winner', u'martin', u'childs', u'shakespeare', u'in', u'love', u'production', u'design', u'turns', u'the', u'original', u'prague', u'surroundings', u'into', u'one', u'creepy', u'place', u'even', u'the', u'acting', u'in', u'from', u'hell', u'is', u'solid', u'with', u'the', u'dreamy', u'depp', u'turning', u'in', u'typically', u'strong', u'performance', u'and', u'deftly', u'handling', u'british', u'accent', u'ians', u'holm', u'joe', u'goulds', u'secret', u'and', u'richardson', u'102', u'dalmatians', u'log', u'in', u'great', u'supporting', u'roles', u'but', u'the', u'big', u'surprise', u'here', u'is', u'graham', u'cringed', u'the', u'first', u'time', u'she', u'opened', u'her', u'mouth', u'imagining', u'her', u'attempt', u'at', u'an', u'irish', u'accent', u'but', u'it', u'actually', u'wasnt', u'half', u'bad', u'the', u'film', u'however', u'is', u'all', u'good', u'00', u'for', u'strong', u'violencegore', u'sexuality', u'language', u'and', u'drug', u'content']\n",
        "N-gram data string:[u'films', u'adapted', u'comic', u'books', u'plenty', u'success', u'theyre', u'superheroes', u'batman', u'superman', u'spawn', u'geared', u'kids', u'casper', u'arthouse', u'crowd', u'ghost', u'world', u'theres', u'really', u'comic', u'book', u'like', u'hell', u'starters', u'created', u'alan', u'moore', u'eddie', u'campbell', u'brought', u'medium', u'new', u'level', u'mid', u'80s', u'12part', u'series', u'called', u'watchmen', u'say', u'moore', u'campbell', u'thoroughly', u'researched', u'subject', u'jack', u'ripper', u'like', u'saying', u'michael', u'jackson', u'starting', u'look', u'little', u'odd', u'book', u'graphic', u'novel', u'500', u'pages', u'long', u'includes', u'nearly', u'30', u'consist', u'footnotes', u'words', u'dont', u'dismiss', u'film', u'source', u'past', u'comic', u'book', u'thing', u'stumbling', u'block', u'hells', u'directors', u'albert', u'allen', u'hughes', u'getting', u'hughes', u'brothers', u'direct', u'ludicrous', u'casting', u'carrot', u'riddle', u'better', u'direct', u'film', u'thats', u'set', u'ghetto', u'features', u'really', u'violent', u'street', u'crime', u'mad', u'geniuses', u'menace', u'ii', u'society', u'ghetto', u'question', u'course', u'whitechapel', u'1888', u'londons', u'east', u'end', u'filthy', u'sooty', u'place', u'whores', u'called', u'unfortunates', u'starting', u'little', u'nervous', u'mysterious', u'psychopath', u'carving', u'profession', u'surgical', u'precision', u'stiff', u'turns', u'copper', u'peter', u'godley', u'robbie', u'coltrane', u'world', u'calls', u'inspector', u'frederick', u'abberline', u'johnny', u'depp', u'blow', u'crack', u'case', u'abberline', u'widower', u'prophetic', u'dreams', u'unsuccessfully', u'tries', u'quell', u'copious', u'amounts', u'absinthe', u'opium', u'arriving', u'whitechapel', u'befriends', u'unfortunate', u'named', u'mary', u'kelly', u'heather', u'graham', u'say', u'isnt', u'proceeds', u'investigate', u'horribly', u'gruesome', u'crimes', u'police', u'surgeon', u'stomach', u'dont', u'think', u'needs', u'briefed', u'jack', u'ripper', u'wont', u'particulars', u'say', u'moore', u'campbell', u'unique', u'interesting', u'theory', u'identity', u'killer', u'reasons', u'chooses', u'slay', u'comic', u'dont', u'bother', u'cloaking', u'identity', u'ripper', u'screenwriters', u'terry', u'hayes', u'vertical', u'limit', u'rafael', u'yglesias', u'les', u'mis', u'rables', u'good', u'job', u'keeping', u'hidden', u'viewers', u'end', u'funny', u'watch', u'locals', u'blindly', u'point', u'finger', u'blame', u'jews', u'indians', u'englishman', u'capable', u'committing', u'ghastly', u'acts', u'hells', u'ending', u'whistling', u'stonecutters', u'song', u'simpsons', u'days', u'holds', u'electric', u'carwho', u'steve', u'guttenberg', u'star', u'dont', u'worry', u'itll', u'make', u'sense', u'hells', u'appearance', u'certainly', u'dark', u'bleak', u'surprising', u'looks', u'like', u'tim', u'burton', u'film', u'planet', u'apes', u'did', u'times', u'like', u'sleepy', u'hollow', u'print', u'saw', u'wasnt', u'completely', u'finished', u'color', u'music', u'finalized', u'comments', u'marilyn', u'manson', u'cinematographer', u'peter', u'deming', u'dont', u'say', u'word', u'ably', u'captures', u'dreariness', u'victorianera', u'london', u'helped', u'make', u'flashy', u'killing', u'scenes', u'remind', u'crazy', u'flashbacks', u'twin', u'peaks', u'violence', u'film', u'pales', u'comparison', u'blackandwhite', u'comic', u'oscar', u'winner', u'martin', u'childs', u'shakespeare', u'love', u'production', u'design', u'turns', u'original', u'prague', u'surroundings', u'creepy', u'place', u'acting', u'hell', u'solid', u'dreamy', u'depp', u'turning', u'typically', u'strong', u'performance', u'deftly', u'handling', u'british', u'accent', u'ians', u'holm', u'joe', u'goulds', u'secret', u'richardson', u'102', u'dalmatians', u'log', u'great', u'supporting', u'roles', u'big', u'surprise', u'graham', u'cringed', u'time', u'opened', u'mouth', u'imagining', u'attempt', u'irish', u'accent', u'actually', u'wasnt', u'half', u'bad', u'film', u'good', u'00', u'strong', u'violencegore', u'sexuality', u'language', u'drug', u'content', u'films adapted', u'adapted comic', u'comic books', u'books plenty', u'plenty success', u'success theyre', u'theyre superheroes', u'superheroes batman', u'batman superman', u'superman spawn', u'spawn geared', u'geared kids', u'kids casper', u'casper arthouse', u'arthouse crowd', u'crowd ghost', u'ghost world', u'world theres', u'theres really', u'really comic', u'comic book', u'book like', u'like hell', u'hell starters', u'starters created', u'created alan', u'alan moore', u'moore eddie', u'eddie campbell', u'campbell brought', u'brought medium', u'medium new', u'new level', u'level mid', u'mid 80s', u'80s 12part', u'12part series', u'series called', u'called watchmen', u'watchmen say', u'say moore', u'moore campbell', u'campbell thoroughly', u'thoroughly researched', u'researched subject', u'subject jack', u'jack ripper', u'ripper like', u'like saying', u'saying michael', u'michael jackson', u'jackson starting', u'starting look', u'look little', u'little odd', u'odd book', u'book graphic', u'graphic novel', u'novel 500', u'500 pages', u'pages long', u'long includes', u'includes nearly', u'nearly 30', u'30 consist', u'consist footnotes', u'footnotes words', u'words dont', u'dont dismiss', u'dismiss film', u'film source', u'source past', u'past comic', u'comic book', u'book thing', u'thing stumbling', u'stumbling block', u'block hells', u'hells directors', u'directors albert', u'albert allen', u'allen hughes', u'hughes getting', u'getting hughes', u'hughes brothers', u'brothers direct', u'direct ludicrous', u'ludicrous casting', u'casting carrot', u'carrot riddle', u'riddle better', u'better direct', u'direct film', u'film thats', u'thats set', u'set ghetto', u'ghetto features', u'features really', u'really violent', u'violent street', u'street crime', u'crime mad', u'mad geniuses', u'geniuses menace', u'menace ii', u'ii society', u'society ghetto', u'ghetto question', u'question course', u'course whitechapel', u'whitechapel 1888', u'1888 londons', u'londons east', u'east end', u'end filthy', u'filthy sooty', u'sooty place', u'place whores', u'whores called', u'called unfortunates', u'unfortunates starting', u'starting little', u'little nervous', u'nervous mysterious', u'mysterious psychopath', u'psychopath carving', u'carving profession', u'profession surgical', u'surgical precision', u'precision stiff', u'stiff turns', u'turns copper', u'copper peter', u'peter godley', u'godley robbie', u'robbie coltrane', u'coltrane world', u'world calls', u'calls inspector', u'inspector frederick', u'frederick abberline', u'abberline johnny', u'johnny depp', u'depp blow', u'blow crack', u'crack case', u'case abberline', u'abberline widower', u'widower prophetic', u'prophetic dreams', u'dreams unsuccessfully', u'unsuccessfully tries', u'tries quell', u'quell copious', u'copious amounts', u'amounts absinthe', u'absinthe opium', u'opium arriving', u'arriving whitechapel', u'whitechapel befriends', u'befriends unfortunate', u'unfortunate named', u'named mary', u'mary kelly', u'kelly heather', u'heather graham', u'graham say', u'say isnt', u'isnt proceeds', u'proceeds investigate', u'investigate horribly', u'horribly gruesome', u'gruesome crimes', u'crimes police', u'police surgeon', u'surgeon stomach', u'stomach dont', u'dont think', u'think needs', u'needs briefed', u'briefed jack', u'jack ripper', u'ripper wont', u'wont particulars', u'particulars say', u'say moore', u'moore campbell', u'campbell unique', u'unique interesting', u'interesting theory', u'theory identity', u'identity killer', u'killer reasons', u'reasons chooses', u'chooses slay', u'slay comic', u'comic dont', u'dont bother', u'bother cloaking', u'cloaking identity', u'identity ripper', u'ripper screenwriters', u'screenwriters terry', u'terry hayes', u'hayes vertical', u'vertical limit', u'limit rafael', u'rafael yglesias', u'yglesias les', u'les mis', u'mis rables', u'rables good', u'good job', u'job keeping', u'keeping hidden', u'hidden viewers', u'viewers end', u'end funny', u'funny watch', u'watch locals', u'locals blindly', u'blindly point', u'point finger', u'finger blame', u'blame jews', u'jews indians', u'indians englishman', u'englishman capable', u'capable committing', u'committing ghastly', u'ghastly acts', u'acts hells', u'hells ending', u'ending whistling', u'whistling stonecutters', u'stonecutters song', u'song simpsons', u'simpsons days', u'days holds', u'holds electric', u'electric carwho', u'carwho steve', u'steve guttenberg', u'guttenberg star', u'star dont', u'dont worry', u'worry itll', u'itll make', u'make sense', u'sense hells', u'hells appearance', u'appearance certainly', u'certainly dark', u'dark bleak', u'bleak surprising', u'surprising looks', u'looks like', u'like tim', u'tim burton', u'burton film', u'film planet', u'planet apes', u'apes did', u'did times', u'times like', u'like sleepy', u'sleepy hollow', u'hollow print', u'print saw', u'saw wasnt', u'wasnt completely', u'completely finished', u'finished color', u'color music', u'music finalized', u'finalized comments', u'comments marilyn', u'marilyn manson', u'manson cinematographer', u'cinematographer peter', u'peter deming', u'deming dont', u'dont say', u'say word', u'word ably', u'ably captures', u'captures dreariness', u'dreariness victorianera', u'victorianera london', u'london helped', u'helped make', u'make flashy', u'flashy killing', u'killing scenes', u'scenes remind', u'remind crazy', u'crazy flashbacks', u'flashbacks twin', u'twin peaks', u'peaks violence', u'violence film', u'film pales', u'pales comparison', u'comparison blackandwhite', u'blackandwhite comic', u'comic oscar', u'oscar winner', u'winner martin', u'martin childs', u'childs shakespeare', u'shakespeare love', u'love production', u'production design', u'design turns', u'turns original', u'original prague', u'prague surroundings', u'surroundings creepy', u'creepy place', u'place acting', u'acting hell', u'hell solid', u'solid dreamy', u'dreamy depp', u'depp turning', u'turning typically', u'typically strong', u'strong performance', u'performance deftly', u'deftly handling', u'handling british', u'british accent', u'accent ians', u'ians holm', u'holm joe', u'joe goulds', u'goulds secret', u'secret richardson', u'richardson 102', u'102 dalmatians', u'dalmatians log', u'log great', u'great supporting', u'supporting roles', u'roles big', u'big surprise', u'surprise graham', u'graham cringed', u'cringed time', u'time opened', u'opened mouth', u'mouth imagining', u'imagining attempt', u'attempt irish', u'irish accent', u'accent actually', u'actually wasnt', u'wasnt half', u'half bad', u'bad film', u'film good', u'good 00', u'00 strong', u'strong violencegore', u'violencegore sexuality', u'sexuality language', u'language drug', u'drug content']\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We use the vectorizer to transform the raw string data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = vectorizer.fit_transform(X_train)\n",
      "X_test = vectorizer.transform(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We use the multinomial Naive Bayes model, suitable for classification with word counts of Tf-idf. Alpha acts as a smoothing parameter, using 0 for No smoothing and 1 for maximum also be the default value.  By using smoothing we reduce the possibility of overfitting. It should be to account for features not present in learning samples and prevents zero computations. Where alpha < 1 - this is Lidstone smoothing and we refer to alpha = 1 as Laplace smoothing."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nb_classifier = MultinomialNB(alpha=0.5,fit_prior=True).fit(X_train,y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_nb_predicted = nb_classifier.predict(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print test_labels[1:10]\n",
      "print y_nb_predicted[0:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "[1 1 1 1 1 1 1 1 1 1]\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now onto the statistics for our classifier. Here we look at the difference between predicted labels for test data and the actual test data labels. \n",
      "\n",
      "Precision looks at the intersection of documents expected in that label with those predicted, as a fraction to predicted (retrieved) documents\n",
      "\n",
      "![](http://upload.wikimedia.org/math/5/3/1/531de241d25a02032bafe4fbceccf584.png)\n",
      "\n",
      "Recall looks at the same intersection as a fraction to the expected (relevant) documents \n",
      "\n",
      "![](http://upload.wikimedia.org/math/3/c/b/3cb5a8e4492f4b12fa87059b6ee18a80.png)\n",
      "\n",
      "We view F-1 measure as a combination of recall and precision\n",
      "\n",
      "![]\n",
      "(http://upload.wikimedia.org/math/8/1/7/81729df4a5d653e8db5d693151e7deb2.png)\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"MODEL: Multinomial Naive Bayes\\n\"\n",
      "\n",
      "print 'The precision for this classifier is ' + str(metrics.precision_score(test_labels, y_nb_predicted))\n",
      "print 'The recall for this classifier is ' + str(metrics.recall_score(test_labels, y_nb_predicted))\n",
      "print 'The f1 for this classifier is ' + str(metrics.f1_score(test_labels, y_nb_predicted))\n",
      "print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(test_labels, y_nb_predicted))\n",
      "\n",
      "print '\\nHere is the classification report:'\n",
      "print classification_report(test_labels, y_nb_predicted) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MODEL: Multinomial Naive Bayes\n",
        "\n",
        "The precision for this classifier is 0.813725490196\n",
        "The recall for this classifier is 0.83\n",
        "The f1 for this classifier is 0.821782178218\n",
        "The accuracy for this classifier is 0.82\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "          0       0.83      0.81      0.82       200\n",
        "          1       0.81      0.83      0.82       200\n",
        "\n",
        "avg / total       0.82      0.82      0.82       400\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cm = metrics.confusion_matrix(y_test, y_nb_predicted)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print cm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[162  38]\n",
        " [ 34 166]]\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Using NLTK Naive Bayes Clasffifier"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This once again uses movie reviews for classification, but instead we use the NLTK Naive Bayes implementation. The issue with this is that it doesn't allow us to set a priory level nor an alpha, hence we can't tweak it as much."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Stopwords are words we wish to filter out of a text document. As we have seen using SkiLearn's, I was able to pass this as a parameter to the vectorizer. However with NLTK we cannot do this, so instead will create a list of words. These are usually common words seen in movie reviews which we don't believe should be any factor in classification. For example 'film' and 'movie' are clearly the subject of the text so what information do we obtain by keeping the word. Other common english words are often removed such as 'i' and 'the'."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stopwords = ['film', 'movie', 'i', 'the', 'and', 'because', 'of', 'the', 'is','in','on','for','with','this','but','there','what','when','in','into' ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Part of learning from data is cleaning the data. Dirty data can cause bad or biased results. Hence we must give text data a basic cleaning, which standaridises the text. Here we lower text, remove punctuation and any new lines, tabs. The list can go on."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cleanWord(word):\n",
      "    word = word.lower()\n",
      "    word = re.sub('\\\\n|\\\\t',' ',word)\n",
      "    return word.translate(string.maketrans(\"\",\"\"), string.punctuation)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once again we collect up the documents words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "documents = [(list(movie_reviews.words(fileid)), category)\n",
      "        for category in movie_reviews.categories()\n",
      "        for fileid in movie_reviews.fileids(category)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Randomly shuffling the documents can assist us in cross-validation. This is when we re-run the tests, usually 10 times, on the same set of data but use different training and test sets. Its to ensure our results are consistent on whatever part of the data the classifier is run on."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "random.shuffle(documents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we begin to create the features used to train the classifier. Rather than  using tfidf we are going to look at the documents with respect to the highest frequency words across all documents.\n",
      "\n",
      "To do this, we use NLTK FreqDist which gives an ordered hash map of each word as the key along with its occurance, as the value."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import FreqDist\n",
      "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print all_words[1:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The issue with creating the frequency distribution straight from the words is that we include punctuation and a lot of junk. Hence below we can use the method cleanWord whilst iterating through all words. This allows us to create a cleaner freqdist and cleaner documents. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clean_words = []\n",
      "clean_docs = []\n",
      "for word_list,label in documents:\n",
      "    doc = []\n",
      "    for word in word_list:\n",
      "        word = cleanWord(word)\n",
      "        if word not in stopwords and len(word) >= 2 :\n",
      "            clean_words.append(word)\n",
      "            doc.append(word)\n",
      "    clean_docs.append((doc,label))\n",
      "    \n",
      "fd = FreqDist(clean_words) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.85\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now using this information we must extract features from each document. Here we define our extractor to look at top 2000 most frequent words. If the document contains a top word, then its corresponding entry in a hashmap is made 'True', else it is 'False'"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_features = all_words.keys()[:2000]\n",
      "\n",
      "def document_features(document):\n",
      "    document_words = set(document)\n",
      "    features = {}\n",
      "    for word in word_features:\n",
      "        features['contains(%s)' % word] = (word in document_words)\n",
      "    return features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can create the feature sets which involves calling document_features on each document. Then we split the data into training and test data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "featuresets = [(document_features(d), c) for (d,c) in clean_docs]\n",
      "train_set, test_set = featuresets[100:], featuresets[:100]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "NLTK is great, although its algorithms aren't as good as SkiLearn, with the variety of Naive Bayes one can use, so here we simply use NaiveBayesClassifier which takes the training examples, which it assumes are tuples containing (features, labels). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      "print nltk.classify.accuracy(classifier, test_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier.show_most_informative_features(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}